{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- CONFIGURATION ---\nmodel_name = \"google/mt5-small\"\ndataset_name = \"ai4bharat/samanantar\"\ndataset_config = \"as\"\nmax_length = 128\nbatch_size = 2  # <-- REDUCED FURTHER to 2\ngradient_accumulation_steps = 8  # <-- NEW: Accumulate gradients over 8 steps\nnum_epochs = 1\n\n# --- 1. SETUP & INSTALLS ---\nprint(\"Installing required libraries...\")\n!pip install transformers[sentencepiece] datasets sacrebleu wandb -q > /dev/null\nprint(\"Libraries installed.\")\n\nfrom transformers import MT5ForConditionalGeneration, MT5Tokenizer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom datasets import load_dataset\nimport torch\nimport wandb\n\n# --- 2. LOGIN TO W&B ---\nwandb.login(key='100fa93408aa5a13bfb4acdc7d19060ef991b61a')\n\n# --- 3. LOAD DATA AND MODEL ---\nprint(\"Loading dataset and model...\")\ndataset = load_dataset(dataset_name, dataset_config)\n\n# --- NEW LINE: SUBSAMPLE THE DATA FOR A QUICK TEST ---\ndataset[\"train\"] = dataset[\"train\"].select(range(10000))  # Use only the first 10,000 examples\n\ntokenizer = MT5Tokenizer.from_pretrained(model_name)\nmodel = MT5ForConditionalGeneration.from_pretrained(model_name, use_cache=False)\nprint(\"Dataset and model loaded successfully!\")\n\n# --- 4. PREPROCESS DATA ---\ndef preprocess_function(examples):\n    inputs = [\"translate English to Assamese: \" + en for en in examples['src']]\n    targets = examples['tgt']\n    model_inputs = tokenizer(inputs, text_target=targets, max_length=max_length, truncation=True, padding=\"max_length\")\n    return model_inputs\n\nprint(\"Tokenizing dataset...\")\ntokenized_datasets = dataset.map(preprocess_function, batched=True)\nprint(\"Dataset tokenized.\")\n\n# --- 5. SET UP TRAINING ---\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"no\",\n    save_strategy=\"epoch\",\n    learning_rate=3e-4,\n    per_device_train_batch_size=batch_size, # Very small batch size\n    per_device_eval_batch_size=batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps, # <-- NEW: Accumulate gradients\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=num_epochs,\n    predict_with_generate=True,\n    fp16=True,  # Mixed precision\n    report_to=\"wandb\",\n)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\n# --- 6. START TRAINING! ---\nprint(\"Starting training...\")\ntrain_result = trainer.train()\nprint(\"Training finished!\")\n\n# --- 7. SAVE MODEL ---\ntrainer.save_model(\"my_english_assamese_model\")\ntokenizer.save_pretrained(\"my_english_assamese_model\")\nprint(\"Model saved!\")\n\n# --- 8. LOG METRICS ---\nmetrics = train_result.metrics\ntrainer.log_metrics(\"train\", metrics)\ntrainer.save_metrics(\"train\", metrics)\n\nwandb.finish()\nprint(\"Check your W&B dashboard for results!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T18:36:25.583361Z","iopub.execute_input":"2025-08-23T18:36:25.584037Z","iopub.status.idle":"2025-08-23T18:53:12.370911Z","shell.execute_reply.started":"2025-08-23T18:36:25.584012Z","shell.execute_reply":"2025-08-23T18:53:12.369948Z"}},"outputs":[{"name":"stdout","text":"Installing required libraries...\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"name":"stdout","text":"Libraries installed.\nLoading dataset and model...\n","output_type":"stream"},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'T5Tokenizer'. \nThe class this function is called from is 'MT5Tokenizer'.\n","output_type":"stream"},{"name":"stdout","text":"Dataset and model loaded successfully!\nTokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27b1c50bc89648218f5da4d398ef09bd"}},"metadata":{}},{"name":"stdout","text":"Dataset tokenized.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/4073800610.py:64: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"},{"name":"stdout","text":"Starting training...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='313' max='313' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [313/313 16:27, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training finished!\nModel saved!\n***** train metrics *****\n  epoch                    =        1.0\n  total_flos               =  1231091GF\n  train_loss               =     6.7027\n  train_runtime            = 0:16:30.06\n  train_samples_per_second =       10.1\n  train_steps_per_second   =      0.316\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1321874227200000.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>313</td></tr><tr><td>train_loss</td><td>6.70275</td></tr><tr><td>train_runtime</td><td>990.0644</td></tr><tr><td>train_samples_per_second</td><td>10.1</td></tr><tr><td>train_steps_per_second</td><td>0.316</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">./results</strong> at: <a href='https://wandb.ai/ankurankit51-indian-institute-of-technology/huggingface/runs/pzk0nmqd' target=\"_blank\">https://wandb.ai/ankurankit51-indian-institute-of-technology/huggingface/runs/pzk0nmqd</a><br> View project at: <a href='https://wandb.ai/ankurankit51-indian-institute-of-technology/huggingface' target=\"_blank\">https://wandb.ai/ankurankit51-indian-institute-of-technology/huggingface</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250823_182159-pzk0nmqd/logs</code>"},"metadata":{}},{"name":"stdout","text":"Check your W&B dashboard for results!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}